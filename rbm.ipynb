{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d1ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa12d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    \"\"\"Restricted Boltzmann Machine\"\"\"\n",
    "    def __init__(self, n_visible=4096, n_hidden=400, W=None, hbias=None, vbias=None, n_epochs=100, eta=0.0001, batch_size=128, random_state=42):\n",
    "        \"\"\"\n",
    "        W: shape = [n_hidden, n_visible]\n",
    "        hbias: shape = [n_hidden, ]\n",
    "        vbias: shape = [n_visible, ]\n",
    "        \"\"\"\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "        \n",
    "        self.n_visible = tf.constant(n_visible)\n",
    "        self.n_hidden = tf.constant(n_hidden)\n",
    "        self.n_epochs = tf.constant(n_epochs)\n",
    "        self.eta = tf.constant(eta) # learning rate\n",
    "        self.batch_size = tf.constant(batch_size)\n",
    "        self.eval_ = {'cross_entropy':[], 'pseudo_likelihood':[], 'visible_energy':[]}\n",
    "\n",
    "        if W is None:\n",
    "            W = tf.Variable(tf.random.normal([n_hidden, n_visible],\n",
    "                                             mean=0,stddev=tf.sqrt(2/float(n_hidden+n_visible))), dtype='float32')\n",
    "        if hbias is None:\n",
    "            hbias = tf.Variable(tf.zeros([n_hidden]), dtype='float32')\n",
    "        if vbias is None:\n",
    "            vbias = tf.Variable(tf.zeros([n_visible]), dtype='float32')\n",
    "\n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "\n",
    "    def propup(self, vis):\n",
    "        \"\"\"return p(h|v)\n",
    "        \"\"\"\n",
    "        wxv_c = (tf.matmul(vis, tf.transpose(self.W)) + self.hbias)*2 # [batch_size, n_hidden] \n",
    "        return self._sigmoid(wxv_c)\n",
    "\n",
    "    def v_to_h(self, v0_sample):\n",
    "        \"\"\"v -> h, sample h given v from p(h|v)\n",
    "        \"\"\"\n",
    "        h1_mean = self.propup(v0_sample)\n",
    "        h1_sample = (self.random.binomial(n=1, p=h1_mean, size=h1_mean.shape)-0.5)*2\n",
    "        return tf.Variable(h1_sample,dtype='float32')\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        \"\"\"return p(v|h)\n",
    "        \"\"\"\n",
    "        hxw_b = (tf.matmul(hid, self.W) + self.vbias)*2\n",
    "        return self._sigmoid(hxw_b)\n",
    "\n",
    "    def h_to_v(self, h0_sample):\n",
    "        \"\"\"h -> v, sample v given h from p(v|h)\n",
    "        \"\"\"\n",
    "        v1_mean = self.propdown(h0_sample)\n",
    "        v1_sample = (self.random.binomial(n=1, p=v1_mean, size=v1_mean.shape)-0.5)*2\n",
    "        return tf.Variable(v1_sample,dtype='float32')\n",
    "\n",
    "    def gibbs_update(self, v_sample, k=1):\n",
    "        \"\"\"gibbs sampling\n",
    "        CD-k to obtain batch_size number of visible states\n",
    "        \"\"\"\n",
    "        for step in range(k):\n",
    "            h_sample = self.v_to_h(v_sample)\n",
    "            v_sample = self.h_to_v(h_sample)\n",
    "        return v_sample\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"training of RBM\n",
    "        \"\"\"\n",
    "        m_instances = X.shape[0]\n",
    "        n_batches = m_instances // self.batch_size.numpy()\n",
    "        bar = progressbar.ProgressBar(maxval=n_batches,widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            bar.start()\n",
    "            for batch_index in range(n_batches):\n",
    "                indices = np.random.randint(m_instances, size=self.batch_size) #stochastic gradient descent\n",
    "                v_batch = tf.Variable(X[indices, :],dtype='float32')\n",
    "\n",
    "                # positive phase \n",
    "                with tf.GradientTape(persistent = True) as tp:\n",
    "                    loss = tf.reduce_mean(-tf.reshape(tf.matmul(v_batch, tf.reshape(self.vbias,[self.n_visible,1])), [v_batch.shape[0]]) \n",
    "                                           - tf.reduce_sum( tf.math.log(tf.math.exp(tf.clip_by_value(-(tf.matmul(v_batch,tf.transpose(self.W)) + self.hbias), -50., 50)) + \n",
    "                                                  tf.math.exp(tf.clip_by_value(tf.matmul(v_batch,tf.transpose(self.W)) + self.hbias, -50., 50))), axis = 1 ))\n",
    "                dW = tp.gradient(loss, self.W)\n",
    "                dhbias = tp.gradient(loss, self.hbias)\n",
    "                dvbias = tp.gradient(loss, self.vbias)\n",
    "                del tp\n",
    "\n",
    "                # negative phase\n",
    "                v_sample = self.gibbs_update(v_batch, k=5)\n",
    "                with tf.GradientTape(persistent = True) as tp:\n",
    "                    loss = tf.reduce_mean(-tf.reshape(tf.matmul(v_sample, tf.reshape(self.vbias,[self.n_visible,1])), [v_sample.shape[0]]) \n",
    "                                           - tf.reduce_sum( tf.math.log(tf.math.exp(tf.clip_by_value(-(tf.matmul(v_sample,tf.transpose(self.W)) + self.hbias), -50., 50)) + \n",
    "                                                  tf.math.exp(tf.clip_by_value(tf.matmul(v_sample,tf.transpose(self.W)) + self.hbias, -50., 50))), axis = 1 ))\n",
    "                dW -= tp.gradient(loss, self.W)\n",
    "                dhbias -= tp.gradient(loss, self.hbias)\n",
    "                dvbias -= tp.gradient(loss, self.vbias)\n",
    "                del tp\n",
    "                \n",
    "                self.W.assign_sub(self.eta * dW)\n",
    "                self.hbias.assign_sub(self.eta * dhbias )     \n",
    "                self.vbias.assign_sub(self.eta * dvbias)\n",
    "\n",
    "                bar.update(batch_index+1)\n",
    "            bar.finish()\n",
    "\n",
    "            CE = self.cross_entropy(X)\n",
    "            L = self.pseudo_likelihood(X)\n",
    "            visible_energy = tf.reduce_mean(self.visible_energy(X)).numpy()\n",
    "            self.eval_['cross_entropy'].append(CE)\n",
    "            self.eval_['pseudo_likelihood'].append(L)\n",
    "            self.eval_['visible_energy'].append(visible_energy)      \n",
    "            print('epoch:', epoch, 'cross entropy:', CE, 'pseudo-likelihood:', L, 'visible energy:', visible_energy)\n",
    "        return self\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\" logistic function\n",
    "        \"\"\"\n",
    "        return 1. / (1. + tf.math.exp(-tf.clip_by_value(z, -250, 250)))\n",
    "\n",
    "    def cross_entropy(self,v_input):\n",
    "        \"\"\"return mean of cross entropy\n",
    "        \"\"\"\n",
    "        v_input = tf.Variable(v_input,dtype='float32')\n",
    "        h = self.v_to_h(v_input)\n",
    "        p = self.propdown(h)\n",
    "        \n",
    "        v_input = v_input/2+0.5\n",
    "        J = v_input*tf.math.log(tf.clip_by_value(p,0.000001,1)) + (1-v_input)*tf.math.log(tf.clip_by_value(1-p,0.000001,1))\n",
    "        return -tf.reduce_mean(J).numpy()\n",
    "    \n",
    "    def pseudo_likelihood(self,v_input):\n",
    "        \"\"\"return mean of pseudo-likelihood\n",
    "        \"\"\"\n",
    "        v = []\n",
    "        v_ = []\n",
    "        for i in range(v_input.shape[0]):\n",
    "            idx = np.random.randint(0,v_input.shape[1])\n",
    "            v_copy = v_input[i].copy()\n",
    "            v_copy[idx] *= (-1)\n",
    "            v_.append(v_copy)\n",
    "\n",
    "        e = self.visible_energy(tf.Variable(v_input,dtype='float32'))\n",
    "        e_ = self.visible_energy(tf.Variable(v_,dtype='float32'))\n",
    "        PL = -self.n_visible.numpy() * np.log(self._sigmoid(e_-e).numpy())\n",
    "        return np.mean(PL)\n",
    "        \n",
    "    def visible_energy(self, v_sample):\n",
    "        \"\"\"return batch_size number of visible energy \n",
    "        \"\"\"\n",
    "        v_sample = tf.Variable(v_sample,dtype='float32')\n",
    "        wxv_c =  tf.matmul(v_sample,tf.transpose(self.W)) + self.hbias\n",
    "        vbias_term = tf.reshape(tf.matmul(v_sample, tf.reshape(self.vbias,[self.n_visible,1])), [v_sample.shape[0]])\n",
    "        hidden_term = tf.reduce_sum( tf.math.log(tf.math.exp(tf.clip_by_value(-wxv_c, -50., 50)) + \n",
    "                                                 tf.math.exp(tf.clip_by_value(wxv_c, -50., 50))), axis = 1 )\n",
    "        return -vbias_term-hidden_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed98e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jg481/MCIsingData/'\n",
    "X=np.load(path+'T225.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c6a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cross entropy: 0.43575454 pseudo-likelihood: 1787.8601 visible energy: -2313.8206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 cross entropy: 0.4354165 pseudo-likelihood: 1784.2795 visible energy: -2318.1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 cross entropy: 0.43485507 pseudo-likelihood: 1793.8171 visible energy: -2330.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 cross entropy: 0.43401247 pseudo-likelihood: 1784.7045 visible energy: -2330.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 cross entropy: 0.43264118 pseudo-likelihood: 1798.2748 visible energy: -2344.7876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RBM at 0x7f7900d4fb20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm = RBM(n_visible=4096, n_hidden=400, n_epochs=5, eta=0.0001, batch_size=128, random_state=42)\n",
    "rbm.fit(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
